{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Vectorización\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"name":"stdout","output_type":"stream","text":["['de' 'dia' 'el' 'es' 'gracias' 'hoy' 'martes' 'muchas' 'que']\n"]}],"source":["for document in corpus:\n","    document_terms = document.split()  # Divide el texto en palabras utilizando los espacios como separadores\n","    if document == corpus[0]:\n","        terms = document_terms\n","    else:\n","        terms = np.concatenate((terms, document_terms))\n","terms = np.unique(terms)\n","print(terms)"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['que', 'dia', 'es', 'hoy'], ['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes'], ['martes', 'muchas', 'gracias']]\n"]}],"source":["corpus2 = []\n","for document in corpus:\n","    document_terms = document.split()  # Divide el texto en palabras utilizando los espacios como separadores\n","    corpus2.append(document_terms)\n","print(corpus2)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"name":"stdout","output_type":"stream","text":["   de  dia  el  es  gracias  hoy  martes  muchas  que\n","0   0    1   0   1        0    1       0       0    1\n","1   1    1   1   1        0    1       1       0    0\n","2   0    0   0   0        1    0       1       1    0\n"]},{"name":"stderr","output_type":"stream","text":["/Users/carlosmontiel/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","df = pd.DataFrame(columns=terms)\n","# Tokenizador personalizado que divide por espacios\n","def mi_tokenizador(text):\n","    return text.split()\n","\n","# Inicializar CountVectorizer con el tokenizador personalizado\n","vectorizer = CountVectorizer(binary=True, tokenizer=mi_tokenizador)\n","\n","# Aplicar CountVectorizer a las frases\n","vectores_frases = vectorizer.fit_transform(corpus)\n","\n","# Obtener la representación de las frases en forma de matriz\n","matriz_frases = vectores_frases.toarray()\n","\n","df = pd.DataFrame(vectores_frases.toarray(), columns=terms)\n","\n","print(df)"]},{"cell_type":"markdown","metadata":{},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   de  dia  el  es  gracias  hoy  martes  muchas  que\n","0   0    1   0   1        0    1       0       0    1\n","1   1    1   1   1        0    1       2       0    0\n","2   0    0   0   0        1    0       1       1    0\n"]},{"name":"stderr","output_type":"stream","text":["/Users/carlosmontiel/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}],"source":["# Inicializar CountVectorizer con el tokenizador personalizado\n","vectorizer = CountVectorizer(tokenizer=mi_tokenizador)\n","\n","# Aplicar CountVectorizer a las frases\n","vectores_frases = vectorizer.fit_transform(corpus)\n","\n","# Obtener la representación de las frases en forma de matriz\n","matriz_frases = vectores_frases.toarray()\n","\n","df = pd.DataFrame(vectores_frases.toarray(), columns=terms)\n","\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"name":"stdout","output_type":"stream","text":["  de dia el es gracias hoy martes muchas que\n","0  1   2  1  2       1   2      2      1   1\n"]}],"source":["import math\n","\n","nuevo_registro = {'de':0 , 'dia':0, 'el':0, 'es':0, 'gracias':0, 'hoy':0, 'martes':0, 'muchas':0, 'que':0}\n","\n","dfDF=pd.DataFrame(columns=terms)\n","dfDF = pd.concat([dfDF, pd.DataFrame([nuevo_registro])])\n","for term in terms:\n","    for document in corpus:\n","        termsInDocument = document.split()\n","        if termsInDocument.count(term)>0:\n","            dfDF[term] = dfDF[term]+1\n","print(dfDF)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["         de       dia        el        es   gracias       hoy    martes  \\\n","0  0.477121  0.176091  0.477121  0.176091  0.477121  0.176091  0.176091   \n","\n","     muchas       que  \n","0  0.477121  0.477121  \n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/k9/vb534jt106d2plphjzl86xcw0000gn/T/ipykernel_21628/1113468358.py:4: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n","  dfIDF[term] = math.log10(len(corpus) / dfDF[term])\n"]}],"source":["dfIDF=pd.DataFrame(columns=terms)\n","dfIDF = pd.concat([dfIDF, pd.DataFrame([nuevo_registro])])\n","for term in terms:\n","    dfIDF[term] = math.log10(len(corpus) / dfDF[term])\n","print(dfIDF)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  de dia el es gracias hoy martes muchas que\n","0  0   1  0  1       0   1      0      0   1\n","1  1   1  1  1       0   1      2      0   0\n","2  0   0  0  0       1   0      1      1   0\n"]}],"source":["dfTF=pd.DataFrame(columns=terms)\n","x = 0\n","for document in corpus:\n","    dfTF = pd.concat([dfTF, pd.DataFrame([nuevo_registro])],ignore_index=True)\n","    for term in terms:\n","        termsInDocument = document.split()\n","        dfTF.loc[x,term] = dfTF.loc[x,term]+termsInDocument.count(term)\n","    x=x+1\n","print(dfTF)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["         de       dia        el        es   gracias       hoy    martes  \\\n","0       0.0  0.176091       0.0  0.176091       0.0  0.176091       0.0   \n","1  0.477121  0.176091  0.477121  0.176091       0.0  0.176091  0.352183   \n","2       0.0       0.0       0.0       0.0  0.477121       0.0  0.176091   \n","\n","     muchas       que  \n","0       0.0  0.477121  \n","1       0.0       0.0  \n","2  0.477121       0.0  \n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/k9/vb534jt106d2plphjzl86xcw0000gn/T/ipykernel_21628/1146631753.py:8: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n","  IDF = float(dfIDF[term])\n"]}],"source":["dfTFIDF = pd.DataFrame(columns=terms)\n","x = 0\n","for document in corpus:\n","    dfTFIDF = pd.concat([dfTFIDF, pd.DataFrame([nuevo_registro])],ignore_index=True)\n","    for term in terms:\n","        termsInDocument = document.split()\n","        TF = float(dfTF.loc[x,term])\n","        IDF = float(dfIDF[term])\n","        dfTFIDF.loc[x,term] = TF * IDF \n","    x=x+1\n","print(dfTFIDF)"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (4061235596.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    0---\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["def getDocumentsInOrder(documentIndex, corpus):\n","    dfCS = pd.DataFrame(columns=terms)\n","    x = 0\n","    for document in corpus:\n","        dfCS = pd.concat([dfCS, pd.DataFrame([nuevo_registro])],ignore_index=True)\n","        value = cosine_similarity(document, corpus[documentIndex])\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
