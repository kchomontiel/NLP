{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## LSTM Bot QA"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n","[LINK](http://convai.io/data/)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"bDFC0I3j9oFD"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"cq3YXak9sGHd"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-17 20:18:09.958845: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import one_hot\n","from tensorflow.keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from keras.models import Model\n","from tensorflow.keras.layers import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RHNkUaPp6aYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["El dataset ya se encuentra descargado\n"]}],"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('data_volunteers.json', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n","    output = 'data_volunteers.json'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WZy1-wgG-Rp7"},"outputs":[],"source":["# dataset_file\n","import json\n","\n","text_file = \"data_volunteers.json\"\n","with open(text_file) as f:\n","    data = json.load(f) # la variable data será un diccionario\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ue5qd54S-eew"},"outputs":[{"data":{"text/plain":["dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Observar los campos disponibles en cada linea del dataset\n","data[0].keys()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jHBRAXPl-3dz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cantidad de rows utilizadas: 6033\n"]}],"source":["chat_in = []\n","chat_out = []\n","\n","input_sentences = []\n","output_sentences = []\n","output_sentences_inputs = []\n","max_len = 30\n","\n","def clean_text(txt):\n","    txt = txt.lower()    \n","    txt.replace(\"\\'d\", \" had\")\n","    txt.replace(\"\\'s\", \" is\")\n","    txt.replace(\"\\'m\", \" am\")\n","    txt.replace(\"don't\", \"do not\")\n","    txt = re.sub(r'\\W+', ' ', txt)\n","    \n","    return txt\n","\n","for line in data:\n","    for i in range(len(line['dialog'])-1):\n","        # vamos separando el texto en \"preguntas\" (chat_in)\n","        # y \"respuestas\" (chat_out)\n","        chat_in = clean_text(line['dialog'][i]['text'])\n","        chat_out = clean_text(line['dialog'][i+1]['text'])\n","\n","        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n","            continue\n","\n","        input_sentence, output = chat_in, chat_out\n","        \n","        # output sentence (decoder_output) tiene <eos>\n","        output_sentence = output + ' <eos>'\n","        # output sentence input (decoder_input) tiene <sos>\n","        output_sentence_input = '<sos> ' + output\n","\n","        input_sentences.append(input_sentence)\n","        output_sentences.append(output_sentence)\n","        output_sentences_inputs.append(output_sentence_input)\n","\n","print(\"Cantidad de rows utilizadas:\", len(input_sentences))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"07L1qj8pC_l6"},"outputs":[{"data":{"text/plain":["('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["input_sentences[1], output_sentences[1], output_sentences_inputs[1]"]},{"cell_type":"markdown","metadata":{"id":"8P-ynUNP5xp6"},"source":["### 2 - Preprocesamiento\n","Realizar el preprocesamiento necesario para obtener:\n","- word2idx_inputs, max_input_len\n","- word2idx_outputs, max_out_len, num_words_output\n","- encoder_input_sequences, decoder_output_sequences, decoder_targets"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["MAX_VOCAB_SIZE = 8000"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Palabras en el vocabulario: 1799\n","Sentencia de entrada más larga: 9\n"]}],"source":["from keras.preprocessing.text import Tokenizer\n","\n","# tokenizador de inglés\n","input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n","input_tokenizer.fit_on_texts(input_sentences)\n","input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n","\n","word2idx_inputs = input_tokenizer.word_index\n","print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n","\n","max_input_len = max(len(sen) for sen in input_integer_seq)\n","print(\"Sentencia de entrada más larga:\", max_input_len)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Palabras en el vocabulario: 1806\n","Sentencia de salida más larga: 10\n"]}],"source":["# tokenizador de español\n","# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n","# sacamos los \"<>\" para que no afectar nuestros tokens\n","output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n","output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n","output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n","output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n","\n","word2idx_outputs = output_tokenizer.word_index\n","print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n","\n","num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) \n","# Se suma 1 para incluir el token de palabra desconocida\n","\n","max_out_len = max(len(sen) for sen in output_integer_seq)\n","print(\"Sentencia de salida más larga:\", max_out_len)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["(6033, 10, 1807)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["from keras.utils import to_categorical\n","decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n","decoder_output_sequences = decoder_output_sequences - 1\n","decoder_targets = to_categorical(decoder_output_sequences, num_classes=num_words_output)\n","decoder_targets.shape"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cantidad de rows del dataset: 6033\n","encoder_input_sequences shape: (6033, 9)\n","decoder_input_sequences shape: (6033, 10)\n"]}],"source":["print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n","\n","encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n","print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n","\n","decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n","print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)"]},{"cell_type":"markdown","metadata":{"id":"_CJIsLBbj6rg"},"source":["### 3 - Preparar los embeddings\n","Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["El dataset ya se encuentra descargado\n"]}],"source":["# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n","# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n","# disponibles descargar de la página oficial como se explica en el siguiente bloque\n","import os\n","if os.access('fasttext.pkl', os.F_OK) is False:\n","    !curl -L -o 'fasttext.pkl' 'https://drive.google.com/u/0/uc?id=1Qi1r-u5lsEsNqRSxLrpNOqQ3B_ufltCa&export=download&confirm=t'\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dicctionarios para traducir de embedding a IDX de la palabra\n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    WORD_MAX_SIZE = 60\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["model_embeddings = FasttextEmbeddings()"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["preparing embedding matrix...\n","number of null word embeddings: 1\n"]}],"source":["# Crear la Embedding matrix de las secuencias\n","# en inglés\n","\n","print('preparing embedding matrix...')\n","embed_dim = model_embeddings.N_FEATURES\n","words_not_found = []\n","\n","# word_index provieen del tokenizer\n","\n","nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs))+1 # vocab_size\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word2idx_inputs.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        \n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # words not found in embedding index will be all-zeros.\n","        words_not_found.append(word)\n","\n","print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))"]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 4 - Entrenar el modelo\n","Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_10\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_26 (InputLayer)       [(None, 9)]                  0         []                            \n","                                                                                                  \n"," input_27 (InputLayer)       [(None, 10)]                 0         []                            \n","                                                                                                  \n"," embedding_10 (Embedding)    (None, 9, 300)               540000    ['input_26[0][0]']            \n","                                                                                                  \n"," embedding_11 (Embedding)    (None, 10, 128)              231296    ['input_27[0][0]']            \n","                                                                                                  \n"," lstm_10 (LSTM)              [(None, 128),                219648    ['embedding_10[0][0]']        \n","                              (None, 128),                                                        \n","                              (None, 128)]                                                        \n","                                                                                                  \n"," lstm_11 (LSTM)              [(None, 10, 128),            131584    ['embedding_11[0][0]',        \n","                              (None, 128),                           'lstm_10[0][1]',             \n","                              (None, 128)]                           'lstm_10[0][2]']             \n","                                                                                                  \n"," dense_5 (Dense)             (None, 10, 1807)             233103    ['lstm_11[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 1355631 (5.17 MB)\n","Trainable params: 815631 (3.11 MB)\n","Non-trainable params: 540000 (2.06 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["from keras.models import Model\n","from keras.layers import Input, LSTM, Dense, Embedding\n","from keras.layers import Dropout\n","\n","n_units = 128\n","dropout_rate = 0.2  # Set your desired dropout rate\n","\n","# Define training encoder\n","encoder_inputs = Input(shape=(max_input_len,))\n","encoder_embedding_layer = Embedding(\n","    input_dim=nb_words,\n","    output_dim=embed_dim,\n","    input_length=max_input_len,\n","    weights=[embedding_matrix],\n","    trainable=False)\n","encoder_inputs_x = encoder_embedding_layer(encoder_inputs)\n","\n","encoder = LSTM(n_units, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs_x)\n","encoder_states = [state_h, state_c]\n","\n","# Define training decoder\n","decoder_inputs = Input(shape=(max_out_len,))\n","decoder_embedding_layer = Embedding(\n","    input_dim=num_words_output,\n","    output_dim=n_units,\n","    input_length=max_out_len)\n","decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n","\n","decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n","\n","# Dense\n","decoder_dense = Dense(num_words_output, activation='softmax')\n","dense_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], dense_outputs)\n","\n","model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n","model.summary()\n","\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["# Modelo solo encoder\n","\n","# define inference encoder\n","encoder_model = Model(encoder_inputs, encoder_states)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# Modelo solo decoder (para realizar inferencia)\n","\n","# define inference decoder\n","decoder_state_input_h = Input(shape=(n_units,))\n","decoder_state_input_c = Input(shape=(n_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# En cada predicción habrá una sola palabra de entrada al decoder,\n","# que es la realimentación de la palabra anterior\n","# por lo que hay que modificar el input shape de la layer de Embedding\n","decoder_inputs_single = Input(shape=(1,))\n","decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n","\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","151/151 [==============================] - 8s 53ms/step - loss: 0.8459 - accuracy: 0.8027 - val_loss: 1.7208 - val_accuracy: 0.7392\n","Epoch 2/40\n","151/151 [==============================] - 7s 49ms/step - loss: 0.8380 - accuracy: 0.8051 - val_loss: 1.7319 - val_accuracy: 0.7384\n","Epoch 3/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.8294 - accuracy: 0.8060 - val_loss: 1.7358 - val_accuracy: 0.7359\n","Epoch 4/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.8235 - accuracy: 0.8057 - val_loss: 1.7419 - val_accuracy: 0.7358\n","Epoch 5/40\n","151/151 [==============================] - 8s 55ms/step - loss: 0.8150 - accuracy: 0.8084 - val_loss: 1.7474 - val_accuracy: 0.7381\n","Epoch 6/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.8092 - accuracy: 0.8084 - val_loss: 1.7534 - val_accuracy: 0.7375\n","Epoch 7/40\n","151/151 [==============================] - 7s 44ms/step - loss: 0.8036 - accuracy: 0.8090 - val_loss: 1.7605 - val_accuracy: 0.7374\n","Epoch 8/40\n","151/151 [==============================] - 7s 45ms/step - loss: 0.7962 - accuracy: 0.8110 - val_loss: 1.7654 - val_accuracy: 0.7358\n","Epoch 9/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.7891 - accuracy: 0.8118 - val_loss: 1.7697 - val_accuracy: 0.7367\n","Epoch 10/40\n","151/151 [==============================] - 8s 56ms/step - loss: 0.7841 - accuracy: 0.8109 - val_loss: 1.7806 - val_accuracy: 0.7361\n","Epoch 11/40\n","151/151 [==============================] - 7s 49ms/step - loss: 0.7768 - accuracy: 0.8132 - val_loss: 1.7808 - val_accuracy: 0.7391\n","Epoch 12/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.7700 - accuracy: 0.8133 - val_loss: 1.7864 - val_accuracy: 0.7374\n","Epoch 13/40\n","151/151 [==============================] - 7s 46ms/step - loss: 0.7659 - accuracy: 0.8140 - val_loss: 1.7859 - val_accuracy: 0.7379\n","Epoch 14/40\n","151/151 [==============================] - 7s 46ms/step - loss: 0.7615 - accuracy: 0.8140 - val_loss: 1.7977 - val_accuracy: 0.7374\n","Epoch 15/40\n","151/151 [==============================] - 7s 46ms/step - loss: 0.7535 - accuracy: 0.8173 - val_loss: 1.8058 - val_accuracy: 0.7364\n","Epoch 16/40\n","151/151 [==============================] - 7s 46ms/step - loss: 0.7500 - accuracy: 0.8173 - val_loss: 1.8081 - val_accuracy: 0.7399\n","Epoch 17/40\n","151/151 [==============================] - 7s 46ms/step - loss: 0.7426 - accuracy: 0.8185 - val_loss: 1.8183 - val_accuracy: 0.7385\n","Epoch 18/40\n","151/151 [==============================] - 7s 46ms/step - loss: 0.7377 - accuracy: 0.8185 - val_loss: 1.8190 - val_accuracy: 0.7379\n","Epoch 19/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.7320 - accuracy: 0.8201 - val_loss: 1.8231 - val_accuracy: 0.7355\n","Epoch 20/40\n","151/151 [==============================] - 7s 49ms/step - loss: 0.7269 - accuracy: 0.8211 - val_loss: 1.8314 - val_accuracy: 0.7374\n","Epoch 21/40\n","151/151 [==============================] - 7s 50ms/step - loss: 0.7210 - accuracy: 0.8222 - val_loss: 1.8335 - val_accuracy: 0.7371\n","Epoch 22/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.7177 - accuracy: 0.8223 - val_loss: 1.8454 - val_accuracy: 0.7364\n","Epoch 23/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.7128 - accuracy: 0.8234 - val_loss: 1.8520 - val_accuracy: 0.7366\n","Epoch 24/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.7075 - accuracy: 0.8237 - val_loss: 1.8550 - val_accuracy: 0.7379\n","Epoch 25/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.7035 - accuracy: 0.8248 - val_loss: 1.8626 - val_accuracy: 0.7361\n","Epoch 26/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.6991 - accuracy: 0.8254 - val_loss: 1.8631 - val_accuracy: 0.7379\n","Epoch 27/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.6924 - accuracy: 0.8270 - val_loss: 1.8709 - val_accuracy: 0.7381\n","Epoch 28/40\n","151/151 [==============================] - 8s 54ms/step - loss: 0.6903 - accuracy: 0.8271 - val_loss: 1.8831 - val_accuracy: 0.7367\n","Epoch 29/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.6826 - accuracy: 0.8271 - val_loss: 1.8879 - val_accuracy: 0.7385\n","Epoch 30/40\n","151/151 [==============================] - 8s 50ms/step - loss: 0.6794 - accuracy: 0.8289 - val_loss: 1.8886 - val_accuracy: 0.7368\n","Epoch 31/40\n","151/151 [==============================] - 8s 52ms/step - loss: 0.6749 - accuracy: 0.8296 - val_loss: 1.8943 - val_accuracy: 0.7379\n","Epoch 32/40\n","151/151 [==============================] - 9s 57ms/step - loss: 0.6703 - accuracy: 0.8314 - val_loss: 1.9017 - val_accuracy: 0.7385\n","Epoch 33/40\n","151/151 [==============================] - 8s 55ms/step - loss: 0.6662 - accuracy: 0.8307 - val_loss: 1.9109 - val_accuracy: 0.7354\n","Epoch 34/40\n","151/151 [==============================] - 9s 60ms/step - loss: 0.6604 - accuracy: 0.8330 - val_loss: 1.9178 - val_accuracy: 0.7350\n","Epoch 35/40\n","151/151 [==============================] - 9s 59ms/step - loss: 0.6572 - accuracy: 0.8317 - val_loss: 1.9178 - val_accuracy: 0.7374\n","Epoch 36/40\n","151/151 [==============================] - 8s 56ms/step - loss: 0.6515 - accuracy: 0.8339 - val_loss: 1.9285 - val_accuracy: 0.7333\n","Epoch 37/40\n","151/151 [==============================] - 8s 52ms/step - loss: 0.6482 - accuracy: 0.8359 - val_loss: 1.9333 - val_accuracy: 0.7340\n","Epoch 38/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.6428 - accuracy: 0.8360 - val_loss: 1.9418 - val_accuracy: 0.7351\n","Epoch 39/40\n","151/151 [==============================] - 7s 47ms/step - loss: 0.6409 - accuracy: 0.8364 - val_loss: 1.9477 - val_accuracy: 0.7356\n","Epoch 40/40\n","151/151 [==============================] - 7s 48ms/step - loss: 0.6346 - accuracy: 0.8374 - val_loss: 1.9561 - val_accuracy: 0.7365\n"]}],"source":["hist = model.fit(\n","    [encoder_input_sequences, decoder_input_sequences],\n","    decoder_targets,\n","    epochs=40,\n","    validation_split=0.2\n",")"]},{"cell_type":"markdown","metadata":{"id":"Zbwn0ekDy_s2"},"source":["### 5 - Inferencia\n","Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# Armar los conversores de índice a palabra:\n","idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n","idx2word_target = {v:k for k, v in word2idx_outputs.items()}"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def translate_sentence(input_seq):\n","    # Se transforma la sequencia de entrada a los estados \"h\" y \"c\" de la LSTM\n","    # para enviar la primera vez al decoder\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = word2idx_outputs['<sos>']\n","\n","    # Se obtiene el índice que finaliza la inferencia\n","    eos = word2idx_outputs['<eos>']\n","    \n","    output_sentence = []\n","    for _ in range(max_out_len):\n","        # Predicción del próximo elemento\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","        idx = np.argmax(output_tokens[0, 0, :])\n","\n","        # Si es \"end of sentece <eos>\" se acaba\n","        if eos == idx:\n","            break\n","\n","        # Transformar idx a palabra\n","        word = ''        \n","        if idx > 0:\n","            word = idx2word_target[idx]\n","            output_sentence.append(word)\n","\n","        # Actualizar los estados dada la última predicción\n","        states_value = [h, c]\n","\n","        # Actualizar secuencia de entrada con la salida (re-alimentación)\n","        target_seq[0, 0] = idx\n","\n","    return ' '.join(output_sentence)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 35ms/step\n","-\n","Input: hello how are you \n","Response: you oh so do\n"]}],"source":["i = np.random.choice(len(input_sentences))\n","input_seq = encoder_input_sequences[i:i+1]\n","translation = translate_sentence(input_seq)\n","print('-')\n","print('Input:', input_sentences[i])\n","print('Response:', translation)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: Do you read?\n","1/1 [==============================] - 0s 27ms/step\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","Response: you play years assholes assholes assholes assholes\n"]}],"source":["input_test = \"Do you read?\"\n","print('Input:', input_test)\n","integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n","encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n","translation = translate_sentence(encoder_sequence_test)\n","print('Response:', translation)"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: Do you have any pet?\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","Response: in study assholes assholes assholes assholes assholes\n"]}],"source":["input_test = \"Do you have any pet?\"\n","print('Input:', input_test)\n","integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n","encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n","translation = translate_sentence(encoder_sequence_test)\n","print('Response:', translation)"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: Where are you from?\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","Response: you a had or did assholes assholes assholes\n"]}],"source":["input_test = \"Where are you from?\"\n","print('Input:', input_test)\n","integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n","encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n","translation = translate_sentence(encoder_sequence_test)\n","print('Response:', translation)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
